---
title: "Data624 Final Project"
author: "Jagdish Chhabria, Stephen Haslett"
date: "12/06/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Load required libraries.
library(tidyverse)
library(caret)
library(kableExtra)
library(lubridate)
library(Hmisc)
library(ggplot2)
library(ggthemes)
library(mice)
library(inspectdf)

# Disable scientific numbers for readability purposes.
options(scipen = 999)
```

## Assignment Overview

This is role playing. I am your new boss. I am in charge of production at ABC Beverage and you are a team of data scientists reporting to me. My leadership has told me that new regulations are requiring us to understand our manufacturing process, the predictive factors and be able to report to them our predictive model of PH.

Please use the historical data set I am providing. Build and report the factors in BOTH a technical and non-technical report. I like to use Word and Excel. Please provide your non-technical report in a business friendly readable document and your predictions in an Excel readable format. The technical report should show clearly the models you tested and how you selected your final approach.

Please submit both Rpubs links and .rmd files or other readable formats for technical and non-technical reports. Also submit the excel file showing the prediction of your models for pH.


### Data Importation

```{r dataImport, eval=TRUE, message=FALSE, warning=FALSE}
# Import the StudentData CSV file.
studentDataTraining <- read.csv('./data/StudentData.csv', stringsAsFactors = FALSE)

# Import the StudentEvaluation CSV file.
studentEvaluationTest <- read.csv('./data/StudentEvaluation.csv', stringsAsFactors = FALSE)
```

\ 

### Data Exploration and Cleansing

#### Training Dataset

Firstly, we will take a look at the first few observations in the dataset so we can get a feel for the data. We will then explore the structure of the data using the _str()_ function which will tell us how many observations and variables it contains, and whether or not it contains missing values.

```{r trainingDataStructure, eval=TRUE, message=FALSE, warning=FALSE}
# Take a look at the structure of the training dataset.
head(studentDataTraining, 40) %>% kable() %>% kable_styling() %>% scroll_box(width = '100%', height = '600px')
```

```{r trainingDataMeta, eval=TRUE, message=FALSE, warning=FALSE}
str(studentDataTraining)
```


The results of running the training data through the _str()_ function reveal that the dataset consists of **33 _Variables_**, and **2571 _Observations_**. Almost all of the variables are numerical, with the exception of the **Brand.Code** variable which is categorical. An other important revelation is that some of the variables contain missing values. 

In order to figure out how we should deal with the missing variables, we will now check to see how many missing variables exist.

\ 

```{r trainingDataMissing, eval=TRUE, message=FALSE, warning=FALSE}
# Count the amount of NA values in the dataset.
colSums(is.na(studentDataTraining))
```

\ 

**30** out of **33** variables contain missing values of varying quantities (_ranging from 212 to 1_). This is enough to justify imputation. Rather than removing entire observations with missing values and jeopardizing the accuracy of the data, we will use the **mice** package's _mice()_ function to impute them.

The mice package offers an array of imputation methods (_Predictive mean matching, mean, norm, to name a few_), but due to the fact that the dataset contains both numeric and categorical variables, we have decided to use the **Predictive mean matching** method as this covers both variable types.

```{r trainingDataImputation, eval=TRUE, message=FALSE, warning=FALSE}
# Impute missing values using the Predictive mean matching imputation method.
studentDataTraining <- mice(studentDataTraining, m = 1, method = 'pmm', print = FALSE) %>% complete()

# After imputation, check if any missing values remain.
colSums(is.na(studentDataTraining))
```


As per the above results, we can confirm that the missing values have been eliminated after imputation.

\ 

#### Test Dataset

We will now perform the same inspections/conversions on the test dataset. 

```{r testgDataStructure, eval=TRUE, message=FALSE, warning=FALSE}
# Take a look at the structure of the training dataset.
head(studentEvaluationTest, 40) %>% kable() %>% kable_styling() %>% scroll_box(width = '100%', height = '600px')
```


```{r testDataMeta, eval=TRUE, message=FALSE, warning=FALSE}
str(studentEvaluationTest)
```


The above results reveal that the dataset consists of **33 _Variables_**, and **267 _Observations_**. Some of the variables contain missing values, and so we will impute these using the **_mice()_** funtion and the **Predictive mean matching** method.

```{r testDataImputation, eval=TRUE, message=FALSE, warning=FALSE}
# Impute missing values using the Predictive mean matching imputation method.
studentEvaluationTest <- mice(studentEvaluationTest, m = 1, method = 'pmm', print = FALSE) %>% complete()

# After imputation, check if any missing values remain.
colSums(is.na(studentEvaluationTest))
```

Again, the above results confirm that the missing values have been eliminated after imputation.


#### Data Distribution

Now that the data is in better shape, we will take a look at the distribution of data.

**High Level Training Data Distrubution**

```{r trainingDataDistribution, eval=TRUE, message=FALSE, warning=FALSE}
summary(studentDataTraining)
```

We can see from the above results that some of the variables contain negative values suggesting that some outliers exist in the data. To confirm this, we will use the _inspectdf_ package's _inspect_num()_ function. 

**High Level Test Data Distrubution**
```{r testDataDistribution, eval=TRUE, message=FALSE, warning=FALSE}
summary(studentEvaluationTest)
```


We can see from the above results that some of the vriables contain negative values. 