---
title: "Data624 Final Project"
author: "Jagdish Chhabria, Stephen Haslett"
date: "12/06/2021"
output:
 html_document:
    toc: true
    toc_float: true
    code_folding: hide
    theme: flatly
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Load required libraries.
library(caret)
library(tidyr)
library(dplyr)
library(tidyverse)
library(kableExtra)
library(VIM)
library(lubridate)
library(Hmisc)
library(ggplot2)
library(AppliedPredictiveModeling)
library(ggthemes)
library(skimr)
library(naniar)
library(corrplot)
library(DataExplorer)
library(usdm)
library(psych)
library(cowplot)
library(mice)
library(inspectdf)
library(DMwR2)
library(Cubist)
# Disable scientific numbers for readability purposes.
options(scipen = 999)
```

## Assignment Overview / Problem Statement

This is role playing. I am your new boss. I am in charge of production at ABC Beverage and you are a team of data scientists reporting to me. My leadership has told me that new regulations are requiring us to understand our manufacturing process, the predictive factors and be able to report to them our predictive model of PH.

Please use the historical data set I am providing. Build and report the factors in BOTH a technical and non-technical report. I like to use Word and Excel. Please provide your non-technical report in a business friendly readable document and your predictions in an Excel readable format. The technical report should show clearly the models you tested and how you selected your final approach.

Please submit both Rpubs links and .rmd files or other readable formats for technical and non-technical reports. Also submit the excel file showing the prediction of your models for pH.


## Data Import

There are two files provided:

1) **_StudentData.xlsx_**: This is the training dataset. Note the PH column will be our target we are trying to predict.

2) **_StudentEvaluation.xlsx_**: This is the evaluation dataset. Note the PH column is empty in this dataset.

```{r dataImport, eval=TRUE, message=FALSE, warning=FALSE}
# Load the ABC Beverages' train dataset.
beverage.train <- read.csv('./data/StudentData.csv', na.strings = c('', 'NA'), stringsAsFactors = FALSE)

#  Load the ABC Beverages' evaluation dataset. 
beverage.eval <- read.csv('./data/StudentEvaluation.csv', na.strings = c('', 'NA'), stringsAsFactors = FALSE)
```

\ 


```{r loadData, eval=TRUE, message=FALSE, warning=FALSE}
# Remove the empty PH column from the evaluation data.
beverage.eval <- beverage.eval %>% dplyr::select(-PH)
```

## Data Exploration

### Training Dataset

Firstly, we will take a look at the first few observations in the dataset so we can get a feel for the data. We will then explore the structure of the data using the _str()_ function which will tell us how many observations and variables it contains, and whether or not it contains missing values.

```{r trainingDataStructure, eval=TRUE, message=FALSE, warning=FALSE}
# Take a look at the structure of the training dataset.
head(beverage.train, 40) %>% kable() %>% kable_styling() %>% scroll_box(width = '100%', height = '600px')
```

```{r trainingDataMeta, eval=TRUE, message=FALSE, warning=FALSE}
# Examine the structure of the training data.
str(beverage.train)
```


The results of running the training data through the _str()_ function reveal that the dataset consists of **33 _Variables_**, and **2571 _Observations_**. Almost all of the variables are numerical, with the exception of the **Brand.Code** variable which is categorical. An other important revelation is that some of the variables contain missing values. 


\ 
 

### Test Dataset

We will now perform the same inspections/conversions on the test dataset. 

```{r testgDataStructure, eval=TRUE, message=FALSE, warning=FALSE}
# Examine the structure of the evaluation dataset.
head(beverage.eval, 40) %>% kable() %>% kable_styling() %>% scroll_box(width = '100%', height = '600px')
```


The training dataset contains 32 predictor variables which include 1 categorical variable and the rest are numeric (continuous and discrete) variables. There are 2571 records in the training data and 267 records in the evaluation dataset. The target column is PH. 

The data has the following variables: 

- **Brand Code**: _categorical, values: A, B, C, D_

- **Carb Volume**: _Numeric_

- **Fill Ounces**: _Numeric_

- **PC Volume**: _Numeric_

- **Carb Pressure**: _Numeric_

- **Carb Temp**: _Numeric_

- **PSC**: _Numeric_

- **PSC Fill**: _Numeric_

- **PSC CO2**: _Numeric_

- **Mnf Flow**: _Numeric_

- **Carb Pressure1**: _Numeric_

- **Fill Pressure**: _Numeric_

- **Hyd Pressure1**: _Numeric_

- **Hyd Pressure2**: _Numeric_

- **Hyd Pressure3**: _Numeric_

- **Hyd Pressure4**: _Numeric_

- **Filler Level**: _Numeric_

- **Filler Speed**: _Numeric_

- **Temperature**: _Numeric_

- **Usage cont**: _Numeric_

- **Carb Flow**: _Numeric_

- **Density**: _Numeric_

- **MFR**: _Numeric_

- **Balling**: _Numeric_

- **Pressure Vacuum**: _Numeric_

- **PH**: _This is the numeric **TARGET** variable that has to be predicted_.

- **Bowl Setpoint**: _Numeric_

- **Pressure Setpoint**: _Numeric_

- **Air Pressurer**: _Numeric_

- **Alch Rel**: _Numeric_

- **Carb Rel**: _Numeric_

- **Balling Lvl**: _Numeric_


Now let's check the summary statistics for the data.


```{r checkSummaryStats, echo=FALSE, message=FALSE, warning=FALSE}
summary(beverage.train)
```


```{r checkSummaryStatsEvalData, echo=FALSE, message=FALSE, warning=FALSE}
# Check summary stats for evaluation data
summary(beverage.eval)
```



```{r inspectData, echo=FALSE, message=FALSE, warning=FALSE}
#glimpse(beverage.train)
#dim(beverage.train)
#describe(beverage.train)
introduce(beverage.train)
```



```{r dataSummary, echo=FALSE, message=FALSE, warning=FALSE}
# Display summary statistics
skim(beverage.train)
```



From the above, we see that most of the predictors (except for 2) contain missing data and will therefore need to be imputed.
For the target variable (_PH_), we see that 4 rows are missing this "PH" value. These rows will need to be dropped since they cannot be used for training. 

Let's look at the distribution of the target variable next.

```{r checkTargetVariable, echo=FALSE, message=FALSE, warning=FALSE}
# Plot a histogram of the PH variable.
hist(beverage.train$PH)
```

The above histogram reveals that the target variable is not very skewed, even though there are some outliers.
The minimum value for PH is 7.88 and the maximum value is 9.36 indicating that ABC manufactures relatively alkaline beverages - likley to be green tea or fruit and vegetable juices.


```{r checkMissingData, echo=FALSE, message=FALSE, warning=FALSE}
# Check for missing data.
knitr::kable(miss_var_summary(beverage.train), 
             caption = 'Missing Values',
             format = 'html', 
             table.attr = "style='width:50%;'") %>% 
        kableExtra::kable_styling()
        gg_miss_var(beverage.train)
        gg_miss_upset(beverage.train)
```

From the above, we can see that about 8.25% of the records are missing a value for `MFR`. We may need to drop this feature since as missingness increases, the increasing amount of imputed values would have potential negative consequences. 
The second most missing variable is the categorical variable called "Brand Code", which is missing about 4.67% percent of its values. These could potentially be a 5th brand besides the existing A,B,C and D or could be one of the existing 4 brands. In any case, we will create a new feature category 'Unknown' for these records. The rest of the predictors are missing smaller percentages of values, and we can use KNN imputation for these records. 


```{r plotPredictors, fig.height = 10, fig.width = 10, echo=FALSE}
# Prepare data for plotting predictors.
beverage_df <- beverage.train %>% drop_na() %>%
  dplyr::select(-c(PH, 'Brand.Code')) %>%
  gather(key = 'variable', value = 'value')

# Plot histogram for each predictor.
ggplot(beverage_df) +
  geom_histogram(aes(x=value, y = ..density..), bins = 30) +
  geom_density(aes(x=value), color='red') +
  facet_wrap(. ~variable, scales='free', ncol = 4)

```

From the above plots, we can see that a lot of the predictors are significantly skewed, suggesting that we might need to transform the data. Several features are discrete with limited possible values, e.g. `Pressure Setpoint`. We also see a number of bimodal variables such as `Carb Flow`, `Balling`, and `Balling Level`.


### Boxplots

We now use boxplots to check the spread of each predictor.

```{r plotBoxplots, fig.height = 10, fig.width = 10, echo=FALSE}
# Prepare the data for boxplots.
beverage_df <- beverage.train %>% 
  dplyr::select(-c(PH, 'Brand.Code')) %>%
  tidyr::drop_na() %>%
  gather(key = 'variable', value = 'value')

# Plot the boxplots for each predictor.
beverage_df %>%
  ggplot() + 
  geom_boxplot(aes(x = variable, y = value)) + 
  facet_wrap(. ~variable, scales = 'free', ncol = 6)
```

The boxplots reveal outliers, but we don't have have a strong reason to impute or drop them from the dataset. 

### Predictor-Target Correlations

We now derive the correlations for the numeric predictors. This would enable us to focus on those predictors that show stronger positive or negative correlations with `PH`. Predictors with correlations closer to zero will likely not provide any meaningful information for the target variable.

```{r retainNumericPredictors, echo=FALSE, message=FALSE, warning=FALSE}

beverage_features <- beverage.train %>% dplyr::select(-c('Brand.Code'))
beverage_features <- beverage_features %>% drop_na()
PH<-data.frame(beverage_features$PH)
beverage_features <- beverage_features%>% dplyr::select(-c('PH'))
num_predictors <- ncol(beverage_features)

```



```{r calculateCorrelations, echo=FALSE, message=FALSE, warning=FALSE}

# Show feature correlations/target by decreasing correlation.
stack(sort(cor(PH, beverage_features[,1:num_predictors])[,],decreasing = TRUE))
#stack(sort(cor(beverage_features[, num_predictors + 1], beverage_features[,1:num_predictors])[,],decreasing = TRUE))
```


From the above, we can see that the variables `Bowl Setpoint`, `Filler Level`, `Carb Flow`, `Pressure Vacuum`, and `Carb Rel` have the strongest positive correlations with `PH`, while `Mnf Flow`, `Usage cont`, `Fill Pressure`, `Pressure Setpoint`, and `Hyd Pressure3` have the strongest negative correlations with `PH`. The other features have a weak or slightly negative correlation, which implies they have less predictive power.


### Multicollinearity 

One problem that can occur with multiple regression and other models is a correlation between predictors or multicollinearity. A quick check is to run correlations between all predictors.


```{r plotCorrelations, echo=FALSE, fig.height=8, fig.width=10 }
# Plot pairwise correlations between Predictors.
corr.bev <-cor(beverage_features, use = "pairwise.complete.obs", method = 'pearson')
corrplot(corr.bev, method="color", type = "lower", tl.col = "black", tl.srt = 5)

#corrplot(corr.bev, method = "color",type = "upper", order = "original", number.cex = .6,addCoef.col = "black", tl.srt = 90, diag = TRUE)
```


We can see that some predictors are highly correlated with one another, such as `Balling Level` and `Carb Volume`, `Carb Rel` and `Alch Rel`, `Density`, and `Balling`, with a correlation between 0.75 and 1. When we start examining predictors for our models, we'll have to consider the correlations between them and avoid including pairs with strong correlations.

In general, it looks like many of the predictors go hand-in-hand with other features and multicollinearity could be a problem.


```{r analyzePredictorCorrelations,echo=FALSE, message=FALSE, warning=FALSE}
# Analyze multi-collinearity.
vifcor(beverage_features) 

```


The vifcor function from the usdm package allows us to do an early analysis into multi-collinearity. As can be seen from the above, this function tells us that 6 of the 31 numeric predictors are highly correlated.


### Near-Zero Variance Predictors

Lastly, we want to check for any features that show near zero-variance. Predictors that are the same across most of the instances will add little predictive information.

```{r checkZeroVar, echo=FALSE, message=FALSE, warning=FALSE}
nzv <- nearZeroVar(beverage.train, saveMetrics = TRUE)
nzv[nzv$nzv,][1:5,] %>% drop_na()

#str(nzv, vec.len = 2)
```

Since "Hyd Pressure1" displays near-zero variance, we will drop this feature prior to modeling.


## 2. Data Preparation

To summarize our data preparation and exploration, we distinguish our findings into a few categories below.

### Removed Fields

-   `MFR` has more than 8% missing values, so we can remove this predictor.
-   `Hyd Pressure1` shows little variance, so we can remove this predictor.

```{r removeTwoVariablesWithMissingData, echo=FALSE, message=FALSE, warning=FALSE}
# Remove the 2 fields from our training data.
beverage.train.clean <- beverage.train %>% dplyr::select(-c(MFR, 'Hyd.Pressure1'))

# Remove the 2 fields from our evaluation data.
beverage.eval.clean <- beverage.eval %>% dplyr::select(-c(MFR, 'Hyd.Pressure1'))
```


-   We had 4 rows with missing `PH` that need to be removed.
-   We replace missing values for `Brand Code` with "Unknown".
-   Impute remaining missing values using `kNN()` from the `VIM` package
-   We then impute remaining missing values using `kNN()` from the `VIM` package.

```{r removeRowsWithMissingTarget, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(200)

# Drop rows with missing PH values.
beverage.train.clean <- beverage.train.clean %>% filter(!is.na(PH))
```


```{r convertMissingBrandCode, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(200)

# Change Brand Code missing to 'Unknown' in our training data.
brand_code <- beverage.train.clean %>% dplyr::select('Brand.Code') %>% replace_na(list('Brand.Code' = 'Unknown'))
beverage.train.clean$'Brand.Code' <- brand_code$'Brand.Code'

# Change Brand Code missing to 'Unknown' in our evaluation data.
brand_code <- beverage.eval.clean %>% dplyr::select('Brand.Code') %>% replace_na(list('Brand.Code' = 'Unknown'))
beverage.eval.clean$'Brand.Code' <- brand_code$'Brand.Code'
```


\ 

### Imputing Missing Values

**30** out of **33** variables contain missing values of varying quantities (_ranging from 212 to 1_). This is enough to justify imputation. Rather than removing entire observations with missing values and jeopardizing the accuracy of the data, we will use the **mice** package's _mice()_ function to impute them.

The mice package offers an array of imputation methods (_Predictive mean matching, mean, norm, to name a few_), but due to the fact that the dataset contains both numeric and categorical variables, we have decided to use the **Predictive mean matching** method as this covers both variable types.

```{r missingDataImputation, eval=TRUE, message=FALSE, warning=FALSE}
set.seed(200)

# Impute missing values in training data using the Predictive mean matching imputation method.
beverage.train.clean <- mice(beverage.train.clean, m = 1, method = 'pmm', print = FALSE) %>% complete()

# After imputation, check if any missing values remain.
colSums(is.na(beverage.train.clean))

# Impute missing values in test data using the Predictive mean matching imputation method.
beverage.eval.clean <- mice(beverage.eval.clean, m = 1, method = 'pmm', print = FALSE) %>% complete()

```

As per the above results, we can confirm that the missing values have been eliminated after imputation.

```{r recheckMissingValues, echo=FALSE, message=FALSE, warning=FALSE}
# Re-check for missing values.
plot_missing(beverage.train.clean)
plot_missing(beverage.eval.clean)
```


### Convert Categorical Variable to Dummy variables

"Brand.Code" is a categorical variable with values A, B, C, D and Unknown. So we will convert it to a set of dummy variables for modeling.

```{r convertBrandCodeToDummy, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(200)

# Convert Brand Code to dummy variables in the training data.
bev.train<-dummyVars("~ Brand.Code", data = beverage.train.clean)
dummies<-predict(bev.train, beverage.train.clean)

# Get dummy column names.
dummy.columns <- sort(colnames(dummies))

# Sort the new dummy columns.
dummies<-as.tibble(dummies) %>% dplyr::select(dummy.columns)

# Remove the original categorical variable.
beverage.train.clean <- beverage.train.clean %>% dplyr::select(-'Brand.Code')

# Add the new dummy columns to the training dataframe.
beverage.train.clean <- cbind(dummies, beverage.train.clean)

# Convert Brand Code to dummy variables in the test data.
bev.eval <- dummyVars(" ~ Brand.Code", data = beverage.eval.clean)

# Define the "eval_dummies" variable.
eval_dummies <- predict(bev.eval, beverage.eval.clean)

# Now sort the eval_dummy columns so they match the training set dummies.
eval.dummy.cols <- sort(colnames(eval_dummies))
eval_dummies <- as.tibble(eval_dummies) %>% dplyr::select(eval.dummy.cols)

# Remove the original categorical variable.
beverage.eval.clean <- beverage.eval.clean %>% dplyr::select(-c('Brand.Code'))

# Add the new dummy columns to the test dataframe.
beverage.eval.clean <- cbind(eval_dummies, beverage.eval.clean)
```

### Transform predictors with skewed distributions

As discussed earlier, some of the predictors are highly skewed. To address this, we scale, center, and apply the Box-Cox transformation to thm using the "preProcess" function from the "caret" package. These transformations should result in more normal distributions.

```{r transformPredictors, echo=FALSE, fig.height=14, fig.width=8, message=FALSE, warning=FALSE}
set.seed(200)

# Drop the target variable PH, since it doesn't need transformation
bev.train.predictors <- beverage.train.clean %>% dplyr::select(-c(PH))

# The test data doesn't have the PH column
bev.eval.predictors <- beverage.eval.clean

# Use caret pre-processing to handle scaling, normalizing and BoxCox transforming the training data.
preProcValues <- preProcess(bev.train.predictors, method = c("center", "scale", "BoxCox"))

bev.train.transformed <- predict(preProcValues, bev.train.predictors)
bev.train.transformed$PH <- beverage.train.clean$PH

preProcValues <- preProcess(bev.eval.predictors, method = c("center", "scale", "BoxCox"))
bev.eval.transformed <- predict(preProcValues, bev.eval.predictors)

preProcValues
```

Here are some plots to demonstrate the changes in distributions after the transformations:

```{r fig.height = 10, fig.width = 10}
# Prepare data for ggplot.
gather_df <- bev.train.transformed %>% dplyr::select(-c(PH)) %>% gather(key = 'variable', value = 'value')

# Histogram plots of each variable.
ggplot(gather_df) + geom_histogram(aes(x=value, y = ..density..), bins = 30) +
  geom_density(aes(x = value), color = 'red') +
  facet_wrap(. ~variable, scales = 'free', ncol = 4)
```

As expected, the plots of the dummy variables are binary. For the others, we can still see bimodal predictors since we did not apply any feature engineering to them. Some predictors such as 'PSC Fill' and 'Temperature' still show some skewness. But we can move on building the models.


### Pre-Modeling Data Splitting 

We perform a train-test split with a 80:20 ratio.

```{r premodelingDataSplitting, eval=TRUE, message=FALSE, warning=FALSE}
# Split the training data into train and test sets using an 80% data split.
trainingData <- createDataPartition(bev.train.transformed$PH, p = 0.8, list = FALSE)

# Training data splits.
trainingDataSet <- bev.train.transformed[trainingData, ]
xTrainData <- subset(trainingDataSet, select = -PH)
yTrainData <- subset(trainingDataSet, select = PH)

# Test data splits.
testDataSet <- bev.train.transformed[-trainingData, ]
xTestData <- subset(testDataSet, select = -PH)
yTestData <- subset(testDataSet, select = PH)
```


## Model Building/Fitting 

In this section, we will build and run 3 categories of models: **tree**, **linear**, and **non-linear**. We will then compare the results of the models in each category, select the best category performer, and then select the overall best performer.  

### Non-Linear Models

In the non-linear category, we will build and run 2 models - a **Support Vector Machine (SVM)** model, and a **K-Nearest Neighbors (KNN)** model. We will use the caret package's _train()_ function to build the models, and use the same training and test datasets for both models.

#### Support Vector Machine (SVM) Model 

```{r svmModel, eval=TRUE, message=FALSE, warning=FALSE}
set.seed(200)

# Define the SVM model.
svmModel = train(x = xTrainData, 
                 y = yTrainData$PH,
                 preProcess = c('center', 'scale'),
                 method = 'svmRadial', 
                 tuneLength = 10,
                 trControl = trainControl(method = 'repeatedcv'))

# Run predict() and postResample() on the model and display the results.
svmPrediction <- predict(svmModel, newdata = xTestData)
svmPerformance <- postResample(pred = svmPrediction, obs = yTestData$PH)
svmPerformance
```


```{r appendResultsSVM, eval=TRUE, message=FALSE, warning=FALSE}
# Predict on test data and calculate performance
results<-data.frame()
results <- data.frame(t(postResample(pred = svmPrediction, obs = yTestData$PH))) %>%mutate(Model = "SVM") %>% 
rbind(results)

```


\ 

#### K Nearest Neighbors (KNN) Model

```{r knnModel, eval=TRUE, message=FALSE, warning=FALSE}
set.seed(200)

# Define the KNN model.
knnModel <- train(x = xTrainData,
                  y = yTrainData$PH,
                  preProcess = c('center', 'scale'),
                  method = 'knn',
                  tuneLength = 10)

# Run predict() and postResample() on the model and display the results.
knnPrediction <- predict(knnModel, newdata = xTestData)
knnPerformance <- postResample(pred = knnPrediction, obs = yTestData$PH)
knnPerformance
```


```{r appendResultsKNN,eval=TRUE, message=FALSE, warning=FALSE}
# Predict on test data and calculate performance
results <- data.frame(t(postResample(pred = knnPrediction, obs = yTestData$PH))) %>% mutate(Model = "k-Nearest Neighbors(kNN)") %>% rbind(results)

```


\ 

### Linear Models

In the linear model category, we will build and run a **generalized linear model (GLM)**, and a **partial least squares (PLS)** model.

#### Generalized Linear Model (GLM)

```{r glmModel, eval=TRUE, message=FALSE, warning=FALSE}
set.seed(200)

# Define the GLM model.
glmModel = train(PH ~ .,
                 data = trainingDataSet, 
                 metric = 'RMSE',
                 preProcess = c('center', 'scale'),
                 method = 'glm', 
                 trControl = trainControl(method = 'cv', number = 5, savePredictions = TRUE))

# Run predict() and postResample() on the model and display the results.
glmModelPrediction <- predict(glmModel, xTestData)
glmPerformance <- postResample(pred = glmModelPrediction, obs = yTestData$PH)
glmPerformance
```



```{r appendResultsGLM,eval=TRUE, message=FALSE, warning=FALSE}
# Predict on test data and calculate performance
results <- data.frame(t(postResample(pred = glmModelPrediction, obs = yTestData$PH))) %>% mutate(Model = "Generalized Linear Model(GLM)") %>% rbind(results)

```


\ 

#### Partial Least Squares Model (PLS)

```{r plmModel, eval=TRUE, message=FALSE, warning=FALSE}
set.seed(200)

# Define the PLS model.
plsModel = train(PH ~ .,
                 data = trainingDataSet, 
                 metric = 'RMSE',
                 preProcess = c('center', 'scale'),
                 method = 'pls', 
                 trControl = trainControl(method = 'cv', number = 5, savePredictions = TRUE))

# Run predict() and postResample() on the model and display the results.
plsModelPrediction <- predict(plsModel, xTestData)
plsPerformance <- postResample(pred = plsModelPrediction, obs = yTestData$PH)
plsPerformance
```



```{r appendResultsPLS,eval=TRUE, message=FALSE, warning=FALSE}
# Predict on test data and calculate performance
results <- data.frame(t(postResample(pred = plsModelPrediction, obs = yTestData$PH))) %>% mutate(Model = "Partial Least Squares(PLS)") %>% rbind(results)

```

\ 

### Tree Models

In this category, we will build and run a **cubist** model, and a **single tree** model.

#### Cubist Model

```{r cubistModel, eval=TRUE, message=FALSE, warning=FALSE}
set.seed(200)

# Define the Cubist model.
cubistModel <- cubist(xTrainData, 
                      yTrainData$PH, 
                      committees = 6)

# Run predict() and postResample() on the model and display the results.
cubistModelPrediction <- predict(cubistModel, newdata = xTestData)
cubistPerformance <- postResample(pred = cubistModelPrediction, obs = yTestData$PH)
cubistPerformance
```



```{r appendResultsCubist,eval=TRUE, message=FALSE, warning=FALSE}
# Predict on test data and calculate performance
results <- data.frame(t(postResample(pred = cubistModelPrediction, obs = yTestData$PH))) %>% mutate(Model = "Tree Model(Cubist)") %>% rbind(results)

```

\ 

#### Single Tree Model

```{r singleTreeModel, eval=TRUE, message=FALSE, warning=FALSE}
set.seed(100)

# Define the Single Tree model.
singleTreeModel <- train(xTrainData,
                         yTrainData$PH,
                         method = 'rpart2',
                         tuneLength = 10,
                         trControl = trainControl(method = 'cv'))

# Run predict() and postResample() on the model and display the results.
singleTreeModelPrediction <- predict(singleTreeModel, newdata = xTestData)
singleTreePerformance <- postResample(pred = singleTreeModelPrediction , obs = yTestData$PH) 
singleTreePerformance
```



```{r appendResultsSingleTree, eval=TRUE, message=FALSE, warning=FALSE}
# Predict on test data and calculate performance
results <- data.frame(t(postResample(pred = singleTreeModelPrediction, obs = yTestData$PH))) %>% mutate(Model = "Single Tree Model") %>% rbind(results)

```

\ 

## Model Comparisons 

After running our models, we will now compare the results of the models and select the best performing model within each category. This will allow us to select the overall best performing model.  

#### Non-Linear Model Comparisons

```{r nonlinearComparisons, eval=TRUE, message=FALSE, warning=FALSE}
nonLinearComparisons <- rbind(
  'Support Vector Machine' = svmPerformance,
  'K Nearest Neighbors' = knnPerformance)

nonLinearComparisons %>% kable() %>% kable_styling(bootstrap_options = c('striped'))
```


Using **RMSE** and **Rsquared** as the selection criteria for the best performing model, the **support vector machine** model yielded the best performance. The **_Rsquared_** value of the model is 0.57 which tells us that the model explains **_57%_** of the variability in the data. This trumps the Rsquared value of the KNN model (52%), but not by much.

\ 

#### Linear Model Comparisons

```{r linearComparisons, eval=TRUE, message=FALSE, warning=FALSE}
linearComparisons <- rbind(
  'Generalized Linear Model' = glmPerformance,
  'Partial Least Squares' = plsPerformance)

linearComparisons %>% kable() %>% kable_styling(bootstrap_options = c('striped'))
```

Again, using **RMSE** and **Rsquared** to select the best model, the GLM and PLS models are almost the same in terms of their performance. However, the **generalized linear model** performs slightly better than the **partial least squares** model. The GLM model explains **_41%_** of the data variance which is higher than the Rsquared value of the PLS model by a fraction.


\ 

#### Tree Model Comparisons

```{r treeComparisons, eval=TRUE, message=FALSE, warning=FALSE}
treeComparisons <- rbind(
  'Cubist' = cubistPerformance,
  'Single Tree' = singleTreePerformance)

treeComparisons %>% kable() %>% kable_styling(bootstrap_options = c('striped'))
```

Finally, In the tree model category, based on the fact that the **_cubist_** model has a lower RMSE than that of the **_single tree_** model, and the fact that it explains **_61%_** of the data variance (_as opposed to the single tree model's 45%_), the cubist tree model is the best performing model in this category.


### Model Summary

We now consolidate the results from all the models the following criteria: root mean squared error (RMSE), R-squared, and mean absolute error. The table below lists these criteria for each model.

```{r combinePerformanceMetrics, eval=TRUE, message=FALSE, warning=FALSE}
results %>% dplyr::select(Model, RMSE, Rsquared, MAE)

```

## Model Selection and top predictors analysis

**Based on the RMSE and RSquared values of all the models we ran, the Cubist model is the overall best performer. This can be expected given that this model is more tolerant of multi-collinearity and works well with non-linear features. The Rsquared for the Cubist model tells us that it explains 61% of the data variance which falls within an acceptable RSquared value range. Based on this, we will proceed with the Cubist model as the best predictive model for this project**.

Let's inspect the predictors that this model found important

```{r checkFeatureImportance, eval=TRUE, message=FALSE, warning=FALSE}
var.imp.cubist<-varImp(cubistModel, scale = FALSE)
var.imp.cubist

``` 


```{r plotFeatureImportance, eval=TRUE, message=FALSE, warning=FALSE}

#plot(varImp(cubistModel, scale = FALSE), top=20)
#plot(varImp(svmModel, scale = FALSE), top=20)

```

Interestingly, we can see that the list of important predictors has some that had strong correlations (positive and negative) with the target variable. For example: Alch Rel, Bowl Setpoint, Carb Flow, Pressure Vacuum, Oxygen Filler and Mnf Flow.

At the same time, there are other predictors that showed strong correlation with PH, but did not make it to the top 10 important predictors. For example: Filler Level, Carb Rel, Oxygen Filler, Usage cont, Fill Pressure, Temperature, Pressure Setpoint, Hyd Pressure2 and Hyd Pressure3.

Instead, the topmost important predictors had variables such as Balling Lvl, Bowl Setpoint, Filler Speed and Balling in the important predictors list that did not demonstrate the strongest correlations. 

This begins to make more sense when we compare to the predictor-predictor correlations calculated previously as well as the results of the vifcor function used previously. We and see that Carb Rel and Alch Rel are strongly correlated as are Alch Rel and Hyd Pressure3. So this indicates that the model is taking into account multi-collinearity and avoiding predictors that are strongly correlated with others that have already been selected and thereby do not provide incremental predictive power.


\ 

## Predictions

Now that we have identified the **_Cubist_** model as the best predictive model, we will apply the model to the evaluation dataset by replacing the empty **_PH** values in the evaluation dataset with the Cubist model's predictions.

```{r finalPHPreictions, eval=TRUE, message=FALSE, warning=FALSE}
# Define the "evaluationDataClean" variable.
evaluationDataClean <- bev.eval.transformed

# Run predict() on the Cubit model.
cubistPredictions <- predict(cubistModel, newdata = evaluationDataClean)

# Replace the empty PH values in the evaluation set with the Cubist predictions.
evaluationDataClean$PH <- round(cubistPredictions,2)

# Take a look at the evaluation data after PH value replacement.
head(evaluationDataClean, 20) %>% kable() %>% kable_styling() %>% scroll_box(width = '100%', height = '600px')

# Save the final results to a CSV file.
write.csv(evaluationDataClean, './data/FinalPHPredictions.csv', row.names=F)
```

Looking at the PH column (_the last column_) in the final data sample above, we can see that the empty PH values have now been replaced with our Cubist model predictions.


```{r inspectRangeOfPredictedValues, eval=TRUE, message=FALSE, warning=FALSE}
# Inspect range of predicted values for evaluation dataset
summary(evaluationDataClean$PH)
histogram(evaluationDataClean$PH)

```

From the above, we can see that the range of the target variable in the evaluation dataset is slightly narrower than its corresponding range in the training data. This gives us confidence that the model seems to work well on unseen data. Obviously the real accuracy metric would be comparing to the actual PH values for the evaluation data, which we did not have access to.

While not perfectly normal, the shape of the distribution of the predicted PH level is reasonably close to normal.

## Conclusion

Based on the exploratory data analysis of the training dataset, we decided to prepare the data - this included dropping some columns and rows, creating a separate "Unknown" group for missing brand codes, creating dummy variables for the single categorical variable i.e. Brand Code, imputing missing variable data and box-cox transforming the predictors to make them less skewed. 

After this, we used 3 categories of models: Linear, Non-linear and Tree-based. We trained 2 models from each category for a total of 6 models. We used a combination of RMSE and R-squared as the performance metrics to decide the final model. We decided to go with the Cubist model because it's metrics were clearly better than the other models. This model has the lowest RMSE and also happens to have the highest r-squared as well. This is not surprising given that these models handle non-linear relationships and multi-collinearity better. This comes across in the list of top predictors selected by this model, as described in a previous section. 

For the final model selected, we see that it considers the following as the top 5 predictors in terms of importance: Mnf.Flow, Alch.Rel, Balling.Lvl, Pressure.Vacuum and Brand Code C. Finding Brand Code as a top predictor is interesting because at the end of the day, Brand is a not a physical/chemical construct that can be linked to PH levels. but we think it must be best encapsulating other chemical features collectively that are in turn helpful in explaining the PH levels. 

We see that the range of the predicted values in the evaluation data is in line with the range of the predicted values in the training data, which gives us confidence that the selected model seems generalizable. Besides, the general shape of the distribution of the predicted values is approximately normal. 

As with any real-world data science process, the logical next step would be to calculate better accuracy metrics by comparing the predicted values to the actual PH values for the evaluation data. Our recommendation to the manager of ABC Beverages would be to go with the Cubist model and put in place an on-going process to keep monitoring the model and fine-tuning in case the model metrics show any deterioration. 



## References
Linear Models with R: Julian Faraway.
Applied Predictive Modeling: Kuhn & Johnson
https://newalbanysmiles.com/ph-values-of-common-beverages/

