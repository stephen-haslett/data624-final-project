---
title: "Data624 Final Project"
author: "Jagdish Chhabria, Stephen Haslett"
date: "12/06/2021"
output:
 html_document:
    toc: true
    toc_float: true
    code_folding: hide
    theme: flatly
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Load required libraries.
library(caret)
library(tidyr)
library(dplyr)
library(tidyverse)
library(kableExtra)
library(VIM)
library(lubridate)
library(Hmisc)
library(ggplot2)
library(AppliedPredictiveModeling)
library(ggthemes)
library(skimr)
library(naniar)
library(corrplot)
library(DataExplorer)
library(usdm)
library(psych)
library(cowplot)
library(mice)
library(inspectdf)
library(DMwR2)
library(Cubist)
# Disable scientific numbers for readability purposes.
options(scipen = 999)
```

## Assignment Overview

This is role playing. I am your new boss. I am in charge of production at ABC Beverage and you are a team of data scientists reporting to me. My leadership has told me that new regulations are requiring us to understand our manufacturing process, the predictive factors and be able to report to them our predictive model of PH.

Please use the historical data set I am providing. Build and report the factors in BOTH a technical and non-technical report. I like to use Word and Excel. Please provide your non-technical report in a business friendly readable document and your predictions in an Excel readable format. The technical report should show clearly the models you tested and how you selected your final approach.

Please submit both Rpubs links and .rmd files or other readable formats for technical and non-technical reports. Also submit the excel file showing the prediction of your models for pH.


## Data Importation

There are two files provided:

1) StudentData.xlsx: This is the training dataset. Note the PH column will be our target we are trying to predict.

2) StudentEvaluation.xlsx: This is the evaluation dataset. Note the PH column is empty in this dataset.

```{r dataImport, eval=TRUE, message=FALSE, warning=FALSE}
# Load the ABC Beverages' train dataset.
beverage.train <- read.csv('./data/StudentData.csv', na.strings = c('', 'NA'), stringsAsFactors = FALSE)

#  Load the ABC Beverages' evaluation dataset. 
beverage.eval <- read.csv('./data/StudentEvaluation.csv', na.strings = c('', 'NA'), stringsAsFactors = FALSE)
```

\ 


```{r loadData, eval=TRUE, message=FALSE, warning=FALSE}
# Remove the empty PH column from the evaluation data.
beverage.eval <- beverage.eval%>% dplyr::select(-PH)
```

## Data Exploration

### Training Dataset

Firstly, we will take a look at the first few observations in the dataset so we can get a feel for the data. We will then explore the structure of the data using the _str()_ function which will tell us how many observations and variables it contains, and whether or not it contains missing values.

```{r trainingDataStructure, eval=TRUE, message=FALSE, warning=FALSE}
# Take a look at the structure of the training dataset.
head(beverage.train, 40) %>% kable() %>% kable_styling() %>% scroll_box(width = '100%', height = '600px')
```

```{r trainingDataMeta, eval=TRUE, message=FALSE, warning=FALSE}
str(beverage.train)
```


The results of running the training data through the _str()_ function reveal that the dataset consists of **33 _Variables_**, and **2571 _Observations_**. Almost all of the variables are numerical, with the exception of the **Brand.Code** variable which is categorical. An other important revelation is that some of the variables contain missing values. 

In order to figure out how we should deal with the missing variables, we will now check to see how many missing variables exist.

\ 
 

### Test Dataset

We will now perform the same inspections/conversions on the test dataset. 

```{r testgDataStructure, eval=TRUE, message=FALSE, warning=FALSE}
# Take a look at the structure of the test dataset.
head(beverage.test, 40) %>% kable() %>% kable_styling() %>% scroll_box(width = '100%', height = '600px')
```


The training dataset contains 32 predictor variables which include 1 categorical variable and the rest are numeric (continuous and discrete) variables. There are 2571 records in the training data and 267 records in the evaluation dataset. The target column is PH. 

The data has the following variables: 

Brand Code: categorical, values: A, B, C, D
Carb Volume:
Fill Ounces:
PC Volume:
Carb Pressure:
Carb Temp:
PSC:
PSC Fill:
PSC CO2:
Mnf Flow:
Carb Pressure1:
Fill Pressure:
Hyd Pressure1:
Hyd Pressure2:
Hyd Pressure3:
Hyd Pressure4:
Filler Level:
Filler Speed:
Temperature:
Usage cont:
Carb Flow:
Density:
MFR:
Balling:
Pressure Vacuum:
PH: This is the TARGET variable that we will try to predict.
Bowl Setpoint:
Pressure Setpoint:
Air Pressurer:
Alch Rel:
Carb Rel:
Balling Lvl:


Now let's check the summary statistics for the data.


```{r checkSummaryStats, echo=FALSE, message=FALSE, warning=FALSE}
summary(beverage.train)
```



```{r inspectData, echo=FALSE, message=FALSE, warning=FALSE}
#glimpse(beverage.train)
#dim(beverage.train)
#describe(beverage.train)
introduce(beverage.train)
```



```{r dataSummary, echo=FALSE, message=FALSE, warning=FALSE}
# Display summary statistics
skim(beverage.train)
```


```{r, echo=FALSE, message=FALSE, warning=FALSE}
#DataExplorer::plot_histogram(beverage.train)
```


From the above, we see that most of the predictors (except for 2) have some missing data. So these would need to be imputed.
For the target variable (PH), we see that 4 rows are missing this "PH" value. These rows will need to be dropped since they cannot be used for training. 

Let's look at the distribution of the target variable next.

```{r checkTargetVariable, echo=FALSE, message=FALSE, warning=FALSE}
# Plot a histogram of the PH variable.
hist(beverage.train$PH)
```

From the above, we can see that the target variable is not very skewed, even though there are some outliers.


```{r checkMissingData, echo=FALSE, message=FALSE, warning=FALSE}
# Check for missing data.
knitr::kable(miss_var_summary(beverage.train), 
             caption = 'Missing Values',
             format = 'html', 
             table.attr = "style='width:50%;'") %>% 
        kableExtra::kable_styling()
        gg_miss_var(beverage.train)
        gg_miss_upset(beverage.train)
```

From the above, we can see that about 8.25% of the records are missing a value for `MFR`. We may need to drop this feature since as missingness increases, the increasing amount of imputed values would have potential negative consequences. 
The second most missing variable is the categorical variable called "Brand Code", which is missing about 4.67% percent of its values. These could potentially be a 5th brand besides the existing A,B,C and D or could be one of the existing 4 brands. In any case, we will create a new feature category 'Unknown' for these records. The rest of the predictors are missing smaller percentages of values, and we can use KNN imputation for these records. 


```{r plotPredictors, fig.height = 10, fig.width = 10, echo=FALSE}
# Prepare data for plotting predictors.
beverage_df <- beverage.train %>% drop_na() %>%
  dplyr::select(-c(PH, 'Brand.Code')) %>%
  gather(key = 'variable', value = 'value')

# Plot histogram for each predictor.
ggplot(beverage_df) +
  geom_histogram(aes(x=value, y = ..density..), bins = 30) +
  geom_density(aes(x=value), color='red') +
  facet_wrap(. ~variable, scales='free', ncol = 4)

```

From the above plots, we can see that a lot of the predictors are significantly skewed, suggesting that we might need to transform the data. Several features are discrete with limited possible values, e.g. `Pressure Setpoint`. We also see a number of bimodal variables such as `Carb Flow`, `Balling`, and `Balling Level`.


### Boxplots

We now use boxplots to check the spread of each predictor.

```{r plotBoxplots, fig.height = 10, fig.width = 10, echo=FALSE}
# Prepare the data for boxplots.
beverage_df <- beverage.train %>% 
  dplyr::select(-c(PH, 'Brand.Code')) %>%
  tidyr::drop_na() %>%
  gather(key = 'variable', value = 'value')

# Plot the boxplots for each predictor.
beverage_df %>%
  ggplot() + 
  geom_boxplot(aes(x = variable, y = value)) + 
  facet_wrap(. ~variable, scales = 'free', ncol = 6)
```

The boxplots reveal outliers, but we don't have have a strong reason to impute or drop them from the dataset. 

### Predictor-Target Correlations

```{r retainNumericPredictors, echo=FALSE, message=FALSE, warning=FALSE}
beverage_features <- beverage.train %>% dplyr::select(-c('Brand.Code'))
beverage_features <- beverage_features %>% drop_na()

num_predictors <- ncol(beverage_features) - 1
```


We now derive the correlations for the numeric predictors. This would enable us to focus on those predictors that show stronger positive or negative correlations with `PH`. Predictors with correlations closer to zero will likely not provide any meaningful information for the target variable.

```{r calculateCorrelations, echo=FALSE, message=FALSE, warning=FALSE}
# Show feature correlations/target by decreasing correlation.
stack(sort(cor(beverage_features[, num_predictors + 1], beverage_features[,1:num_predictors])[,],decreasing = TRUE))
```

From the above, we can see that the variables `Bowl Setpoint`, `Filler Level`, `Carb Flow`, `Pressure Vacuum`, and `Carb Rel` have the strongest positive correlations with `PH`, while `Mnf Flow`, `Usage cont`, `Fill Pressure`, `Pressure Setpoint`, and `Hyd Pressure3` have the strongest negative correlations with `PH`. The other features have a weak or slightly negative correlation, which implies they have less predictive power.

### Multicollinearity

One problem that can occur with multiple regression is a correlation between predictors or multicollinearity. A quick check is to run correlations between all predictors.


```{r plotCorrelations, echo=FALSE, fig.height=8, fig.width=10 }
# Plot pairwise correlations between Predictors.
corr.bev <- cor(beverage_features, use = "pairwise.complete.obs", method = 'pearson')
corrplot(corr.bev, method="color", type = "lower", tl.col = "black", tl.srt = 5)

#corrplot(corr.bev, method = "color",type = "upper", order = "original", number.cex = .6,addCoef.col = "black", tl.srt = 90, diag = TRUE)
```

We can see that some predictors are highly correlated with one another, such as `Balling Level` and `Carb Volume`, `Carb Rel`, `Alch Rel`, `Density`, and `Balling`, with a correlation between 0.75 and 1. When we start examining predictors for our models, we'll have to consider the correlations between them and avoid including pairs with strong correlations.

In general, it looks like many of the predictors go hand-in-hand with other features and multicollinearity will be a problem.


```{r ,echo=FALSE, message=FALSE, warning=FALSE}
#vifcor(beverage_features) 
```

### Near-Zero Variance Predictors

Lastly, we want to check for any features that show near zero-variance. Predictors that are the same across most of the instances will add little predictive information.

```{r checkZeroVar, echo=FALSE, message=FALSE, warning=FALSE}
nzv <- nearZeroVar(beverage.train, saveMetrics = TRUE)
nzv[nzv$nzv,][1:5,] %>% drop_na()

#str(nzv, vec.len = 2)
```

Since "Hyd Pressure1" displays near-zero variance, we will drop this feature prior to modeling.


## 2. Data Preparation

To summarize our data preparation and exploration, we distinguish our findings into a few categories below.

### Removed Fields

-   `MFR` has more than 8% missing values, so we can remove this predictor.
-   `Hyd Pressure1` shows little variance, so we can remove this predictor.

```{r removeTwoVariablesWithMissingData, echo=FALSE, message=FALSE, warning=FALSE}
# Remove the 2 fields from our training data.
beverage.train.clean <- beverage.train %>% dplyr::select(-c(MFR, 'Hyd.Pressure1'))

# Remove the 2 fields from our evaluation data.
beverage.eval.clean <- beverage.eval %>% dplyr::select(-c(MFR, 'Hyd.Pressure1'))
```


-   We had 4 rows with missing `PH` that need to be removed.
-   We replace missing values for `Brand Code` with "Unknown".
-   Impute remaining missing values using `kNN()` from the `VIM` package
-   We then impute remaining missing values using `kNN()` from the `VIM` package.

```{r removeRowsWithMissingTarget, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(200)

# Drop rows with missing PH values.
beverage.train.clean <- beverage.train.clean %>% filter(!is.na(PH))
```


```{r convertMissingBrandCode, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(200)

# Change Brand Code missing to 'Unknown' in our training data.
brand_code <- beverage.train.clean %>% dplyr::select('Brand.Code') %>% replace_na(list('Brand.Code' = 'Unknown'))
beverage.train.clean$'Brand.Code' <- brand_code$'Brand.Code'

# Change Brand Code missing to 'Unknown' in our evaluation data.
brand_code <- beverage.eval.clean %>% dplyr::select('Brand.Code') %>% replace_na(list('Brand.Code' = 'Unknown'))
beverage.eval.clean$'Brand.Code' <- brand_code$'Brand.Code'
```


\ 

### Imputing Missing Values

**30** out of **33** variables contain missing values of varying quantities (_ranging from 212 to 1_). This is enough to justify imputation. Rather than removing entire observations with missing values and jeopardizing the accuracy of the data, we will use the **mice** package's _mice()_ function to impute them.

The mice package offers an array of imputation methods (_Predictive mean matching, mean, norm, to name a few_), but due to the fact that the dataset contains both numeric and categorical variables, we have decided to use the **Predictive mean matching** method as this covers both variable types.

```{r missingDataImputation, eval=TRUE, message=FALSE, warning=FALSE}
set.seed(200)

# Impute missing values in training data using the Predictive mean matching imputation method.
beverage.train.clean <- mice(beverage.train.clean, m = 1, method = 'pmm', print = FALSE) %>% complete()

# After imputation, check if any missing values remain.
colSums(is.na(beverage.train.clean))

# Impute missing values in test data using the Predictive mean matching imputation method.
beverage.eval.clean <- mice(beverage.eval.clean, m = 1, method = 'pmm', print = FALSE) %>% complete()

```

As per the above results, we can confirm that the missing values have been eliminated after imputation.

```{r recheckMissingValues, echo=FALSE, message=FALSE, warning=FALSE}
# Re-check for missing values.
plot_missing(beverage.train.clean)
plot_missing(beverage.eval.clean)
```


### Convert Categorical Variable to Dummy variables

"Brand.Code" is a categorical variable with values A, B, C, D and Unknown. So we will convert it to a set of dummy variables for modeling.

```{r convertBrandCodeToDummy, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(200)

# Convert Brand Code to dummy variables in the training data
bev.train<-dummyVars("~ Brand.Code", data = beverage.train.clean)
dummies<-predict(bev.train, beverage.train.clean)

# Get dummy column names
dummy.columns <- sort(colnames(dummies))

# Sort the new dummy columns
dummies<-as.tibble(dummies)%>%dplyr::select(dummy.columns)

# Remove the original categorical variable
beverage.train.clean<-beverage.train.clean%>%dplyr::select(-'Brand.Code')

# Add the new dummy columns to the training dataframe
beverage.train.clean<-cbind(dummies, beverage.train.clean)

# Convert Brand Code to dummy variables in the test data
bev.eval<-dummyVars(" ~ Brand.Code", data = beverage.eval.clean)
#beverage.eval.clean$PH <- 1
eval_dummies<-predict(bev.eval, beverage.eval.clean)

# Now sort the eval_dummy columns so they match the training set dummies
eval.dummy.cols <- sort(colnames(eval_dummies))
eval_dummies<-as.tibble(eval_dummies)%>%dplyr::select(eval.dummy.cols)

# Remove the original categorical variable
beverage.eval.clean<-beverage.eval.clean%>%dplyr::select(-c('Brand.Code'))

# Add the new dummy columns to the test dataframe
beverage.eval.clean<-cbind(eval_dummies, beverage.eval.clean)

```

### Transform predictors with skewed distributions

As discussed earlier, some of the predictors are highly skewed. To address this, we scale, center, and apply the Box-Cox transformation to thm using the "preProcess" function from the "caret" package. These transformations should result in more normal distributions.

```{r transformPredictors, echo=FALSE, fig.height=14, fig.width=8, message=FALSE, warning=FALSE}
set.seed(200)

# Drop the target variable PH, since it doesn't need transformation
bev.train.predictors <- beverage.train.clean %>% dplyr::select(-c(PH))

# The test data doesn't have the PH column
bev.eval.predictors <- beverage.eval.clean

# Use caret pre-processing to handle scaling, normalizing and BoxCox transforming the training data.
preProcValues <- preProcess(bev.train.predictors, method = c("center", "scale", "BoxCox"))

bev.train.transformed <- predict(preProcValues, bev.train.predictors)
bev.train.transformed$PH <- beverage.train.clean$PH

preProcValues <- preProcess(bev.eval.predictors, method = c("center", "scale", "BoxCox"))
bev.eval.transformed <- predict(preProcValues, bev.eval.predictors)

preProcValues
```

Here are some plots to demonstrate the changes in distributions after the transformations:

```{r fig.height = 10, fig.width = 10}
# Prepare data for ggplot.
gather_df <- bev.train.transformed %>% dplyr::select(-c(PH)) %>% gather(key = 'variable', value = 'value')

# Histogram plots of each variable.
ggplot(gather_df) + geom_histogram(aes(x=value, y = ..density..), bins = 30) +
  geom_density(aes(x = value), color = 'red') +
  facet_wrap(. ~variable, scales = 'free', ncol = 4)
```

As expected, the plots of the dummy variables are binary. For the others, we can still see bimodal predictors since we did not apply any feature engineering to them. Some predictors such as 'PSC Fill' and 'Temperature' still show some skewness. But we can move on building the models.


### Pre-Modeling Data Splitting

```{r premodelingDataSplitting, eval=TRUE, message=FALSE, warning=FALSE}
# Split the training data into train and test sets using an 80% data split.
trainingData <- createDataPartition(bev.train.transformed$PH, p = 0.8, list = FALSE)

# Training data splits.
trainingDataSet <- bev.train.transformed[trainingData, ]
xTrainData <- subset(trainingDataSet, select = -PH)
yTrainData <- subset(trainingDataSet, select = PH)

# Test data splits.
testDataSet <- bev.train.transformed[-trainingData, ]
xTestData <- subset(testDataSet, select = -PH)
yTestData <- subset(testDataSet, select = PH)
```



## Models

In this section, we will build and run 3 categories of models: **tree**, **linear**, and **non-linear**. We will then compare the results of the models in each category, select the best category performer, and then select the overall best performer.  

### Non-Linear Models

In the non-linear category, we will build and run 2 models - a **Support Vector Machine (SVM)** model, and a **K-Nearest Neighbors (KNN)** model. We will use the caret package's _train()_ function to build the models, and use the same training and test datasets for both models.

#### Support Vector Machine (SVM) Model 

```{r svmModel, eval=TRUE, message=FALSE, warning=FALSE}
set.seed(200)

# Define the SVM model.
svmModel = train(x = xTrainData, 
                 y = yTrainData$PH,
                 preProcess = c('center', 'scale'),
                 method = 'svmRadial', 
                 tuneLength = 10,
                 trControl = trainControl(method = 'repeatedcv'))

# Run predict() and postResample() on the model and display the results.
svmPrediction <- predict(svmModel, newdata = xTestData)
svmPerformance <- postResample(pred = svmPrediction, obs = yTestData$PH)
svmPerformance
```


```{r appendResultsSVM, eval=TRUE, message=FALSE, warning=FALSE}
# Predict on test data and calculate performance
results<-data.frame()
results <- data.frame(t(postResample(pred = svmPrediction, obs = yTestData$PH))) %>%mutate(Model = "SVM") %>% 
rbind(results)

```


\ 

#### K Nearest Neighbors (KNN) Model

```{r knnModel, eval=TRUE, message=FALSE, warning=FALSE}
set.seed(200)

# Define the KNN model.
knnModel <- train(x = xTrainData,
                  y = yTrainData$PH,
                  preProcess = c('center', 'scale'),
                  method = 'knn',
                  tuneLength = 10)

# Run predict() and postResample() on the model and display the results.
knnPrediction <- predict(knnModel, newdata = xTestData)
knnPerformance <- postResample(pred = knnPrediction, obs = yTestData$PH)
knnPerformance
```


```{r appendResultsKNN,eval=TRUE, message=FALSE, warning=FALSE}
# Predict on test data and calculate performance
results <- data.frame(t(postResample(pred = knnPrediction, obs = yTestData$PH))) %>% mutate(Model = "k-Nearest Neighbors(kNN)") %>% rbind(results)

```


\ 

### Linear Models

In the linear model category, we will build and run a **generalized linear model (GLM)**, and a **partial least squares (PLS)** model.

#### Generalized Linear Model (GLM)

```{r glmModel, eval=TRUE, message=FALSE, warning=FALSE}
set.seed(200)

# Define the GLM model.
glmModel = train(PH ~ .,
                 data = trainingDataSet, 
                 metric = 'RMSE',
                 preProcess = c('center', 'scale'),
                 method = 'glm', 
                 trControl = trainControl(method = 'cv', number = 5, savePredictions = TRUE))

# Run predict() and postResample() on the model and display the results.
glmModelPrediction <- predict(glmModel, xTestData)
glmPerformance <- postResample(pred = glmModelPrediction, obs = yTestData$PH)
glmPerformance
```



```{r appendResultsGLM,eval=TRUE, message=FALSE, warning=FALSE}
# Predict on test data and calculate performance
results <- data.frame(t(postResample(pred = glmModelPrediction, obs = yTestData$PH))) %>% mutate(Model = "Generalized Linear Model(GLM)") %>% rbind(results)

```


\ 

#### Partial Least Squares Model (PLS)

```{r plmModel, eval=TRUE, message=FALSE, warning=FALSE}
set.seed(200)

# Define the PLS model.
plsModel = train(PH ~ .,
                 data = trainingDataSet, 
                 metric = 'RMSE',
                 preProcess = c('center', 'scale'),
                 method = 'pls', 
                 trControl = trainControl(method = 'cv', number = 5, savePredictions = TRUE))

# Run predict() and postResample() on the model and display the results.
plsModelPrediction <- predict(plsModel, xTestData)
plsPerformance <- postResample(pred = plsModelPrediction, obs = yTestData$PH)
plsPerformance
```



```{r appendResultsPLS,eval=TRUE, message=FALSE, warning=FALSE}
# Predict on test data and calculate performance
results <- data.frame(t(postResample(pred = plsModelPrediction, obs = yTestData$PH))) %>% mutate(Model = "Partial Least Squares(PLS)") %>% rbind(results)

```

\ 

### Tree Models

In this category, we will build and run a **cubist** model, and a **single tree** model.

#### Cubist Model

```{r cubistModel, eval=TRUE, message=FALSE, warning=FALSE}
set.seed(200)

# Define the Cubist model.
cubistModel <- cubist(xTrainData, 
                      yTrainData$PH, 
                      committees = 6)

# Run predict() and postResample() on the model and display the results.
cubistModelPrediction <- predict(cubistModel, newdata = xTestData)
cubistPerformance <- postResample(pred = cubistModelPrediction, obs = yTestData$PH)
cubistPerformance
```



```{r appendResultsCubist,eval=TRUE, message=FALSE, warning=FALSE}
# Predict on test data and calculate performance
results <- data.frame(t(postResample(pred = cubistModelPrediction, obs = yTestData$PH))) %>% mutate(Model = "Tree Model(Cubist)") %>% rbind(results)

```

\ 

#### Single Tree Model

```{r singleTreeModel, eval=TRUE, message=FALSE, warning=FALSE}
set.seed(100)

# Define the Single Tree model.
singleTreeModel <- train(xTrainData,
                         yTrainData$PH,
                         method = 'rpart2',
                         tuneLength = 10,
                         trControl = trainControl(method = 'cv'))

# Run predict() and postResample() on the model and display the results.
singleTreeModelPrediction <- predict(singleTreeModel, newdata = xTestData)
singleTreePerformance <- postResample(pred = singleTreeModelPrediction , obs = yTestData$PH) 
singleTreePerformance
```



```{r appendResultsSingleTree, eval=TRUE, message=FALSE, warning=FALSE}
# Predict on test data and calculate performance
results <- data.frame(t(postResample(pred = singleTreeModelPrediction, obs = yTestData$PH))) %>% mutate(Model = "Single Tree Model") %>% rbind(results)

```

\ 

### Model Comparisons 

After running our models, we will now compare the results of the models and select the best performing model within each category. This will allow us to select the overall best performing model.  

#### Non-Linear Model Comparisons

```{r nonlinearComparisons, eval=TRUE, message=FALSE, warning=FALSE}
nonLinearComparisons <- rbind(
  'Support Vector Machine' = svmPerformance,
  'K Nearest Neighbors' = knnPerformance)

nonLinearComparisons %>% kable() %>% kable_styling(bootstrap_options = c('striped'))
```


Using **RMSE** and **Rsquared** as the selection criteria for the best performing model, the **support vector machine** model yielded the best performance. The **_Rsquared_** value of the model is 0.57 which tells us that the model explains **_57%_** of the variability in the data. This trumps the Rsquared value of the KNN model (52%), but not by much.

\ 

#### Linear Model Comparisons

```{r linearComparisons, eval=TRUE, message=FALSE, warning=FALSE}
linearComparisons <- rbind(
  'Generalized Linear Model' = glmPerformance,
  'Partial Least Squares' = plsPerformance)

linearComparisons %>% kable() %>% kable_styling(bootstrap_options = c('striped'))
```

Again, using **RMSE** and **Rsquared** to select the best model, the GLM and PLS models are almost the same in terms of their performance. However, the **generalized linear model** performs slightly better than the **partial least squares** model. The GLM model explains **_41%_** of the data variance which is higher than the Rsquared value of the PLS model by a fraction.


\ 

#### Tree Model Comparisons

```{r treeComparisons, eval=TRUE, message=FALSE, warning=FALSE}
treeComparisons <- rbind(
  'Cubist' = cubistPerformance,
  'Single Tree' = singleTreePerformance)

treeComparisons %>% kable() %>% kable_styling(bootstrap_options = c('striped'))
```

Finally, In the tree model category, based on the fact that the **_cubist_** model has a lower RMSE than that of the **_single tree_** model, and the fact that it explains **_61%_** of the data variance (_as opposed to the single tree model's 45%_), the cubist tree model is the best performing model in this category.


### Model Summary

We now consolidate the results from all the models the following criteria: root mean squared error (RMSE), R-squared, and mean absolute error. The table below lists these criteria for each model.

```{r combinePerformanceMetrics, eval=TRUE, message=FALSE, warning=FALSE}
results %>% dplyr::select(Model, RMSE, Rsquared, MAE)

```

## Model Selection

**Based on the RMSE and RSquared values of all the models we ran, the Cubist model is the overall best performer. This can be expected given that this model is more tolerant of multi-collinearity and works well with non-linear features. The Rsquared for the Cubist model tells us that it explains 61% of the data variance which falls within an acceptable RSquared value range. Based on this, we will proceed with the Cubist model as the best predictive model for this project**.

Let's inspect the predictors that this model found important

```{r checkFeatureImportance, eval=TRUE, message=FALSE, warning=FALSE}
varImp(cubistModel)
``` 

\ 

## Predictions

Now that we have identified the **_Cubist_** model as the best predictive model, we will apply the model to the evaluation dataset by replacing the empty **_PH** values in the evaluation dataset with the Cubist model's predictions.

```{r finalPHPreictions, eval=TRUE, message=FALSE, warning=FALSE}
# Define the "evaluationDataClean" variable.
evaluationDataClean <- bev.eval.transformed

# Run predict() on the Cubit model.
cubistPredictions <- predict(cubistModel, newdata = evaluationDataClean)

# Replace the empty PH values in the evaluation set with the Cubist predictions.
evaluationDataClean$PH <- round(cubistPredictions,2)

# Take a look at the evaluation data after PH value replacement.
head(evaluationDataClean, 20) %>% kable() %>% kable_styling() %>% scroll_box(width = '100%', height = '600px')

# Save the final results to a CSV file.
write.csv(evaluationDataClean, './data/FinalPHPredictions.csv', row.names=F)
```

Looking at the PH column (_the last column_) in the final data sample above, we can see that the empty PH values have now been replaced with our Cubist model predictions.


## Conclusion

